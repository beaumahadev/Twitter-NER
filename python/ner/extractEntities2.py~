#!/usr/bin/python

import sys
import os
import re
import subprocess
import time

from signal import *

#BASE_DIR = '/home/aritter/twitter_nlp'
#BASE_DIR = os.environ['HOME'] + '/twitter_nlp'
#BASE_DIR = '/homes/gws/aritter/twitter_nlp'
BASE_DIR = 'twitter_nlp.jar'

if os.environ.has_key('TWITTER_NLP'):
    BASE_DIR = os.environ['TWITTER_NLP']

sys.path.append('%s/python' % (BASE_DIR))
sys.path.append('%s/python/ner' % (BASE_DIR))
sys.path.append('%s/hbc/python' % (BASE_DIR))

import Features
import twokenize_wrapper
from LdaFeatures import LdaFeatures
from Dictionaries import Dictionaries
from Vocab import Vocab

sys.path.append('%s/python/cap' % (BASE_DIR))
sys.path.append('%s/python' % (BASE_DIR))
import cap_classifier
import pos_tagger_stdin
import chunk_tagger_stdin

#ner = subprocess.Popen('java -Xmx1024m -cp /homes/gws/aritter/mallet-2.0.6/lib/mallet-deps.jar:/homes/gws/aritter/mallet-2.0.6/class cc.mallet.fst.SimpleTaggerStdin --include-input false --model-file /homes/gws/aritter/twitter_nlp/data/ner/tweets/combined_train2_nocap_100k.model',

def GetNer():
    #return subprocess.Popen('java -Xmx256m -cp %s/mallet-2.0.6/lib/mallet-deps.jar:%s/mallet-2.0.6/class cc.mallet.fst.SimpleTaggerStdin --weights sparse --model-file %s/experiments/domain_transfer/combined_train.model' % (BASE_DIR, BASE_DIR, BASE_DIR),
    return subprocess.Popen('java -Xmx256m -cp %s/mallet-2.0.6/lib/mallet-deps.jar:%s/mallet-2.0.6/class cc.mallet.fst.SimpleTaggerStdin --weights sparse --model-file %s/models/ner/ner.model' % (BASE_DIR, BASE_DIR, BASE_DIR),
                           shell=True,
                           close_fds=True,
                           stdin=subprocess.PIPE,
                           stdout=subprocess.PIPE)

def GetLLda():
    return subprocess.Popen('%s/hbc/models/LabeledLDA_infer_stdin.out %s/hbc/data/combined.docs.hbc %s/hbc/data/combined.z.hbc 100 100' % (BASE_DIR, BASE_DIR, BASE_DIR),
                           shell=True,
                           close_fds=True,
                           stdin=subprocess.PIPE,
                           stdout=subprocess.PIPE)

ner = GetNer()
llda = GetLLda()

#df = Features.DictionaryFeatures('/homes/gws/aritter/twitter_nlp/data/dictionaries')
fe = Features.FeatureExtractor('%s/data/dictionaries' % (BASE_DIR))

#if len(sys.argv) > 1:
#    posTagger = pos_tagger_stdin.PosTagger()
#else:
#    posTagger = None

posTagger = pos_tagger_stdin.PosTagger()
chunkTagger = chunk_tagger_stdin.ChunkTagger()
capClassifier = cap_classifier.CapClassifier()

vocab = Vocab('%s/hbc/data/vocab' % (BASE_DIR))
dictionaries = Dictionaries('%s/data/LabeledLDA_dictionaries3' % (BASE_DIR))
entityMap = {}
i = 0
for line in open('%s/hbc/data/entities' % (BASE_DIR)):
    entity = line.rstrip('\n')
    entityMap[entity] = i
    i += 1
dictMap = {}
i = 1
for line in open('%s/hbc/data/dictionaries' % (BASE_DIR)):
    dictionary = line.rstrip('\n')
    dictMap[i] = dictionary
    i += 1
dict2label = {}
for line in open('%s/hbc/data/dict-label3' % (BASE_DIR)):
    (dictionary, label) = line.rstrip('\n').split(' ')
    dict2label[dictionary] = label

nLines = 1
for line in sys.stdin:
    line = line.rstrip('\n')
    words = twokenize_wrapper.tokenize(line)
    seq_features = []
    tags = []

    goodCap = capClassifier.Classify(words) > 0.9

    # POS Tagging the tweet
    if posTagger:
        pos = posTagger.TagSentence(words)
    else:
        pos = fields[-1].split(' ')

    # Chunking the tweet
    if chunkTagger:
        word_pos = zip(words, [p.split(':')[0] for p in pos])
        chunk = chunkTagger.TagSentence(word_pos)
    else:
        chunk = fields[-1].split(' ')
        
    quotes = Features.GetQuotes(words)
    for i in range(len(words)):
        features = fe.Extract(words, pos, chunk, i, goodCap) + ['DOMAIN=Twitter']
        if quotes[i]:
            features.append("QUOTED")
        seq_features.append(" ".join(features))
    ner.stdin.write("\t".join(seq_features) + "\n")
        
    for i in range(len(words)):
        tags.append(ner.stdout.readline().rstrip('\n').strip(' '))

    features = LdaFeatures(words, tags)
    #print features.entityStrings

    #Extract and classify entities
    for i in range(len(features.entities)):
        type = None
        wids = [str(vocab.GetID(x.lower())) for x in features.features[i] if vocab.HasWord(x.lower())]
        if len(wids) > 0:
            #TODO: get the mapping (need a hash table from entities to ids?)
            entityid = "-1"
            if entityMap.has_key(features.entityStrings[i].lower()):
                entityid = str(entityMap[features.entityStrings[i].lower()])
            labels = dictionaries.GetDictVector(features.entityStrings[i])
            if sum(labels) == 0:
                labels = [1 for x in labels]
            #print "\t".join([entityid, " ".join(wids), " ".join([str(x) for x in labels])])
            llda.stdin.write("\t".join([entityid, " ".join(wids), " ".join([str(x) for x in labels])]) + "\n")
            sample = llda.stdout.readline().rstrip('\n')
            #print "%s ::: %s" % (features.entityStrings[i], sample)
            labels = [dict2label[dictMap[int(x)]] for x in sample[4:len(sample)-8].split(' ')]
            #print labels
            count = {}
            for label in labels:
                count[label] = count.get(label, 0.0) + 1.0
            maxL = None
            maxP = 0.0
            for label in count.keys():
                p = count[label] / float(len(count))
                if p > maxP or maxL == None:
                    maxL = label
                    maxP = p
            #print "%s\t%s" % (maxP, maxL)

            if maxP > 0.4 and maxL != 'NONE':
                tags[features.entities[i][0]] = "B-%s" % (maxL)
                for j in range(features.entities[i][0]+1,features.entities[i][1]):
                    tags[j] = "I-%s" % (maxL)
            else:
                tags[features.entities[i][0]] = "O"
                for j in range(features.entities[i][0]+1,features.entities[i][1]):
                    tags[j] = "O"
        else:
            tags[features.entities[i][0]] = "O"
            for j in range(features.entities[i][0]+1,features.entities[i][1]):
                tags[j] = "O"

    print " ".join(["%s/%s" % (words[x], tags[x]) for x in range(len(words))])

    #seems like there is a memory leak comming from mallet, so just restart it every 1,000 tweets or so
    #if nLines % 1000 == 0:
    if nLines % 500 == 0:
        start = time.time()
        ner.stdin.close()
        ner.stdout.close()
        #if ner.wait() != 0:
        #sys.stderr.write("error!\n")
        #ner.kill()
        os.kill(ner.pid, SIGTERM)       #Need to do this for python 2.4
        ner.wait()
        ner = GetNer()
#        posTagger.tagger.kill()                #NOTE: pos_tagger handles this...
#        posTagger.tagger.wait()
#        posTagger = pos_tagger_stdin.PosTagger()
        sys.stderr.write("%s\t%s\n" % (nLines, time.time() - start))
    nLines += 1
