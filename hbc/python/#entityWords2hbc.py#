#!/usr/bin/python

##############################################################################################3
# tweets2entityDocs.py
#
# Reads in (entity,word) pairs generated on m45 cluster, in addition to dictionaries
##############################################################################################3

import sys
from optparse import OptionParser

from LdaFeatures import LdaFeatures
from Vocab import Vocab
from Dictionaries import Dictionaries

parser = OptionParser()
parser.add_option("--noWords", action="store_false", dest="useWords", default=True)
parser.add_option("--useNoDict", action="store_true", dest="useNoDict", default=False)
(options, args) = parser.parse_args()

dictionaries = Dictionaries('/homes/gws/aritter/twitter_nlp/data/LabeledLDA_dictionaries')
vocab = Vocab()
eOut = open('entities', 'w')
lOut = open('labels', 'w')
dOut = open('dictionaries', 'w')

stop_list = set()
for word in open('/homes/gws/aritter/twitter_nlp/data/dictionaries/english.stop'):
    word = word.rstrip('\n')
    stop_list.add(word)

fNoDictOut = None
fNoDictOutLabels = None
fNoDictOutEntities = None
if not options.useNoDict:
    fNoDictOut = open('noDictOut', 'w')
    fNoDictOutLabels = open('noDictOutLabels', 'w')
    fNoDictOutEntities = open('noDictOutEntities', 'w')
    
prevEntity = None
words = []
for line in sys.stdin:
    line = line.rstrip('\n')
    (entity, word) = line.split('\t')

    ####################################################################
    # OK, we've read in all the words for this entity, now print
    # out "document"
    ####################################################################
    if prevEntity and prevEntity != entity:
        e = prevEntity
        labels = dictionaries.GetDictVector(e)

        #Filter out stopwords
        words = [x for x in words if (x[0] != 't') or not (x.split('=')[1] in stop_list)]
        #sys.stderr.write(str(words) + "\n")

        if not options.useWords:
            words = [x for x in words if x[0] != 't']


        #limit of 500 words per doc
        words = words[0:500]

        ###############################################################################
        #NOTE: For now, only include entities which appear in one or more dictionary
        #      we could modify this to give them membership in all, or no dictionaries
        #      (in LabeledLDA, don't impose any constraints)
        ###############################################################################
        if sum(labels) > 0:
            lOut.write(' '.join([str(x) for x in labels]) + "\n")
            eOut.write("%s\n" % e)
            print ' '.join([str(vocab.GetID(x)) for x in words])
        elif options.useNoDict:
            labels = [1 for x in labels]
            lOut.write(' '.join([str(x) for x in labels]) + "\n")
            eOut.write("%s\n" % e)
            print ' '.join([str(vocab.GetID(x)) for x in words])
        else:
            #Write to file containing entities which don't appear in any dictionaries
            fNoDictOut.write(' '.join([str(vocab.GetID(x)) for x in words]) + "\n")
            labels = [1 for x in labels]
            fNoDictOutEntities.write("%s\n" % e)
            fNoDictOutLabels.write(' '.join([str(x) for x in labels]) + "\n")
            
        words = []

    words.append(word)
    prevEntity = entity
    
vocab.SaveVocab('vocab')

for d in dictionaries.dictionaries:
    dOut.write(d + "\n")
